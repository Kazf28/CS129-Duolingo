{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Duolingo_data_03_07_final_standardized.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1. Loading the Dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuolingo_data_03_07_final_standardized.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m df\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/EDSseminar/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/EDSseminar/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/EDSseminar/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/EDSseminar/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Applications/miniconda3/envs/EDSseminar/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Duolingo_data_03_07_final_standardized.csv'"
     ]
    }
   ],
   "source": [
    "# 1. Loading the Dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(\"Duolingo_data_03_07_final_standardized.csv\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nDataset info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Preprocessing\n",
    "print(\"\\nChecking for missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Convert churn_time_category to categorical target (binary classification)\n",
    "# Let's consider categories 3 and 4 as \"will churn\" (1) and categories 1 and 2 as \"will not churn\" (0)\n",
    "# df['will_churn'] = df['churn_time_category'].apply(lambda x: 1 if x >= 3 else 0)\n",
    "\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(df['churn_time_category'].value_counts())\n",
    "# print(df['1'].value_counts(normalize=True).map(lambda x: f\"{x:.2%}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Selection\n",
    "# Exclude the original target, churn_time (which would cause data leakage), and any other columns we don't want as features\n",
    "features = [col for col in df.columns if col not in ['churn_time_category', 'user_id', 'churn_time']]\n",
    "X = df[features]\n",
    "y = df['churn_time_category']\n",
    "\n",
    "print(\"\\nSelected features:\", features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "print(f\"\\nTraining samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to imbalanced dataset, use stratified train-test split\n",
    "# X contains your features, y contains your target variable (churn_time_category)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.2,                 # 20% of data for testing\n",
    "    random_state=RANDOM_STATE,     # For reproducibility\n",
    "    stratify=y                     # This is the key parameter for stratified sampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify\n",
    "# Before splitting\n",
    "print(\"Original class distribution:\")\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "# After splitting\n",
    "print(\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nTesting set class distribution:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create DMatrix for faster processing\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dtrain \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241m.\u001b[39mDMatrix(X_train, label\u001b[38;5;241m=\u001b[39my_train)\n\u001b[1;32m      3\u001b[0m dtest \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mDMatrix(X_test, label\u001b[38;5;241m=\u001b[39my_test)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Define parameters\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xgb' is not defined"
     ]
    }
   ],
   "source": [
    "# Create DMatrix for faster processing\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Define parameters\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # multiclass classification\n",
    "    'num_class': 3,                # 4 classes (1, 2, 3)\n",
    "    'eta': 0.1,                    # learning rate\n",
    "    'max_depth': 6,                # max depth of trees\n",
    "    'subsample': 0.8,              # subsample ratio\n",
    "    'colsample_bytree': 0.8,       # feature subsample ratio\n",
    "    'min_child_weight': 1,         # min sum of instance weight needed in a child\n",
    "    'eval_metric': 'mlogloss',     # evaluation metric for multiclass\n",
    "    'seed': RANDOM_STATE,          # for reproducibility\n",
    "    'tree_method': 'hist'          # faster tree construction algorithm\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining a basic XGBoost model...\")\n",
    "num_rounds = 100\n",
    "xgb_model = xgb.train(params, dtrain, num_rounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training accuracy\n",
    "y_train_pred = xgb_model.predict(dtrain)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(f\"Training accuracy: {train_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the basic model\n",
    "y_pred = xgb_model.predict(dtest)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nBasic XGBoost model accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['1', '2', '3'],\n",
    "            yticklabels=['1', '2', '3'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Basic XGBoost Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost provides multiple importance measures - use 'gain'\n",
    "importance_gain = xgb_model.get_score(importance_type='gain')\n",
    "\n",
    "# Create dataframe for visualization\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': list(importance_gain.keys()),\n",
    "    'Importance': list(importance_gain.values())\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance (Gain):\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance[:15])  # Show top 15 features\n",
    "plt.title('XGBoost Feature Importance (Gain)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn's XGBClassifier for easier integration with GridSearchCV\n",
    "print(\"\\nPerforming hyperparameter tuning...\")\n",
    "\n",
    "# Create XGBClassifier\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=3,\n",
    "    random_state=RANDOM_STATE,\n",
    "    tree_method='hist',\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "\n",
    "# Setup CV with stratification\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Use a reduced parameter grid for demonstration - in practice, you might want to try more combinations\n",
    "reduced_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 6],\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_clf,\n",
    "    param_grid=reduced_param_grid,  # Use reduced grid for faster computation\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: {:.4f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance for the best model from GridSearchCV\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': best_xgb_model.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance[:15])  # Show top 15 features\n",
    "plt.title('Feature Importance (Best XGBoost Model After Tuning)', fontsize=16)\n",
    "plt.xlabel('Importance', fontsize=14)\n",
    "plt.ylabel('Feature', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top 15 features\n",
    "print(\"Top 15 most important features:\")\n",
    "print(feature_importance.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the tuned model\n",
    "y_pred_tuned = best_xgb_model.predict(X_test)\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "print(f\"\\nTuned XGBoost model accuracy: {accuracy_tuned:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report (Tuned Model):\")\n",
    "print(classification_report(y_test, y_pred_tuned))\n",
    "\n",
    "# Confusion Matrix for tuned model\n",
    "cm_tuned = confusion_matrix(y_test, y_pred_tuned)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_tuned, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['1', '2', '3'],\n",
    "            yticklabels=['1', '2', '3'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Tuned XGBoost Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Binarize the output\n",
    "y_test_bin = label_binarize(y_test, classes=[1, 2, 3])\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "# Get probability predictions\n",
    "y_score = best_xgb_model.predict_proba(X_test)\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Add diagonal reference line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier (AUC = 0.5)')\n",
    "\n",
    "# Plot ROC curve for each class with different colors\n",
    "colors = ['blue', 'red', 'green', 'purple']\n",
    "class_names = ['Churn Category 1', 'Churn Category 2', 'Churn Category 3']\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color=colors[i], lw=2, \n",
    "             label=f'{class_names[i]} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Add labels, title and other visual elements\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('Multiclass ROC Curve (XGBoost)', fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Set axis limits\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Curves\n",
    "print(\"\\nGenerating learning curves...\")\n",
    "\n",
    "# Use StratifiedKFold for CV\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Define train sizes\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Use best XGBoost model from grid search\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_xgb_model, X, y, \n",
    "    cv=cv,  \n",
    "    n_jobs=-1, \n",
    "    train_sizes=train_sizes,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Calculate mean and std\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, 'o-', color='r', label='Training score')\n",
    "plt.plot(train_sizes, test_mean, 'o-', color='g', label='Cross-validation score')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='r')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='g')\n",
    "plt.xlabel('Training Examples')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Learning Curves for XGBoost (Multiclass Classification)')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.ylim([0.4, 1.01])  # Set y-axis limits for better visualization\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:EDSseminar]",
   "language": "python",
   "name": "conda-env-EDSseminar-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
